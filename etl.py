"""
ETL (Extract, Transform, Load) module.

This module implements a simple pipeline for preparing the raw financial price
data and news sentiment data for analysis and modelling. The pipeline
consists of the following steps:

* **Load** the raw CSV files for both the market prices and the news
  sentiment scores.
* **Clean and preprocess** each dataset independently:
  - Market price data are re‑indexed to a daily frequency, missing days
    are forward-filled (or dropped if no prior data exist), and a basic
    technical indicator (daily returns) is computed.
  - News sentiment data are parsed into datetime objects, missing
    sentiments or headlines are handled gracefully, and the sentiments are
    aggregated per day.
* **Merge** the two cleaned datasets on the date field so that each day
  contains both market features and aggregated sentiment features.
* **Save** the merged dataset to ``data/processed/merged_data.csv`` for
  downstream tasks.

All functions in this module include logging to facilitate debugging and
provide visibility into the ETL process. Errors such as missing files
or malformed data will be logged and re‑raised.
"""

from __future__ import annotations

import os
import logging
from typing import Optional
import pandas as pd  # type: ignore


# Configure a module-level logger. Logging configuration is left to the
# application entry point; here we simply get a logger for this module.
logger = logging.getLogger(__name__)

# Define paths to the raw and processed data directories relative to this
# script. ``RAW_DATA_DIR`` and ``PROCESSED_DATA_DIR`` are used as defaults
# by the loader and saver functions.
RAW_DATA_DIR = os.path.join(os.path.dirname(__file__), '..', 'data', 'raw')
PROCESSED_DATA_DIR = os.path.join(os.path.dirname(__file__), '..', 'data', 'processed')


def load_price_data(path: Optional[str] = None) -> pd.DataFrame:
    """Load the raw price data from a CSV file.

    The price data generated by the data ingestion step is expected to
    have a date index and the typical OHLCV columns. This function reads
    the CSV, parses the dates, and returns a DataFrame with the date as
    an index.

    Args:
        path: Optional path to the CSV file. If not provided, defaults
            to ``RAW_DATA_DIR/price_data.csv``.

    Returns:
        A pandas DataFrame with a datetime index.

    Raises:
        FileNotFoundError: If the file does not exist.
        ValueError: If the CSV cannot be parsed.
    """
    if path is None:
        path = os.path.join(RAW_DATA_DIR, 'price_data.csv')
    logger.info("Loading price data from %s", path)
    if not os.path.exists(path):
        logger.error("Price data file does not exist: %s", path)
        raise FileNotFoundError(f"Price data file not found: {path}")
    try:
        df = pd.read_csv(path, parse_dates=[0], index_col=0)
    except Exception as exc:
        logger.exception("Failed to load price data from %s", path)
        raise ValueError(f"Failed to parse price data: {exc}")
    # Ensure the index is named for consistency
    df.index.name = 'date'
    return df


def preprocess_price_data(df: pd.DataFrame) -> pd.DataFrame:
    """Clean and preprocess the raw price data.

    The preprocessing steps include:

    * Converting the index to daily frequency to fill any missing days.
    * Forward filling missing values (which can occur after reindexing).
    * Calculating daily returns based on the ``Close`` column.

    Args:
        df: Raw market price DataFrame with a datetime index.

    Returns:
        A cleaned DataFrame with an additional ``returns`` column and a
        reset index for merging.
    """
    logger.info("Preprocessing price data (%d rows)", len(df))
    # Ensure the index is sorted and unique
    df = df.sort_index()
    # Reindex to daily frequency; this will insert missing dates as NaNs
    df = df.asfreq('D')
    # Forward fill any missing OHLCV values; for the first row use backfill
    df.fillna(method='ffill', inplace=True)
    df.fillna(method='bfill', inplace=True)
    # Compute daily returns; fill the first value with 0
    if 'Close' in df.columns:
        df['returns'] = df['Close'].pct_change().fillna(0)
    else:
        logger.warning("'Close' column not found in price data; skipping returns computation")
        df['returns'] = 0.0
    # Reset index to expose the date as a column. If the index had no name
    # (i.e. was ``None``), ``reset_index`` will create a column named ``index``;
    # rename it to ``date`` for consistency.
    df = df.reset_index()
    if 'index' in df.columns and 'date' not in df.columns:
        df = df.rename(columns={'index': 'date'})
    return df


def load_news_data(path: Optional[str] = None) -> pd.DataFrame:
    """Load the raw news sentiment data from a CSV file.

    The FNSPID news sentiment placeholder file contains at least three
    columns: ``headline``, ``date`` and ``sentiment``. This function reads
    the CSV and parses the ``date`` column into datetime objects.

    Args:
        path: Optional path to the sentiment CSV. Defaults to
            ``RAW_DATA_DIR/fnspid_news_sentiment.csv``.

    Returns:
        A pandas DataFrame with a ``date`` column of dtype datetime.

    Raises:
        FileNotFoundError: If the file does not exist.
        ValueError: If the CSV cannot be parsed.
    """
    if path is None:
        path = os.path.join(RAW_DATA_DIR, 'fnspid_news_sentiment.csv')
    logger.info("Loading news sentiment data from %s", path)
    if not os.path.exists(path):
        logger.error("News sentiment file does not exist: %s", path)
        raise FileNotFoundError(f"News sentiment file not found: {path}")
    try:
        df = pd.read_csv(path)
    except Exception as exc:
        logger.exception("Failed to load news sentiment data from %s", path)
        raise ValueError(f"Failed to parse news sentiment data: {exc}")
    # Ensure required columns are present
    for col in ['date', 'sentiment']:
        if col not in df.columns:
            logger.error("Required column '%s' missing from news data", col)
            raise ValueError(f"Missing required column '{col}' in news data")
    # Parse dates
    df['date'] = pd.to_datetime(df['date'], errors='coerce')
    return df


def preprocess_news_data(df: pd.DataFrame) -> pd.DataFrame:
    """Clean and preprocess the raw news sentiment data.

    Processing steps include:

    * Dropping rows with invalid or missing dates.
    * Filling missing sentiment scores with 0.
    * Aggregating sentiment scores by day (mean sentiment).

    Args:
        df: Raw news DataFrame with columns at least ``date`` and ``sentiment``.

    Returns:
        A DataFrame indexed by ``date`` with a single column ``sentiment``
        representing the aggregated daily sentiment score.
    """
    logger.info("Preprocessing news sentiment data (%d rows)", len(df))
    # Parse dates if they are not already datetime objects
    df['date'] = pd.to_datetime(df['date'], errors='coerce')
    # Drop rows with missing or invalid dates
    df = df.dropna(subset=['date'])
    # Fill missing sentiment values with 0
    df['sentiment'] = df['sentiment'].fillna(0)
    # Convert sentiment to numeric; non‑convertible values become NaN then filled with 0
    df['sentiment'] = pd.to_numeric(df['sentiment'], errors='coerce').fillna(0)
    # Aggregate by date (mean sentiment per day)
    aggregated = df.groupby(df['date'].dt.normalize())['sentiment'].mean().reset_index()
    aggregated.columns = ['date', 'sentiment']
    return aggregated


def merge_data(price_df: pd.DataFrame, news_df: pd.DataFrame) -> pd.DataFrame:
    """Merge the cleaned price and news datasets on the ``date`` column.

    Args:
        price_df: Preprocessed market data with a ``date`` column.
        news_df: Preprocessed news data with a ``date`` column.

    Returns:
        A merged DataFrame containing all columns from both inputs. Missing
        sentiment scores are filled with 0.
    """
    logger.info("Merging price (%d rows) and news (%d rows) data", len(price_df), len(news_df))
    merged = pd.merge(price_df, news_df, how='left', on='date')
    # Fill missing sentiment values with 0 after merge
    if 'sentiment' in merged.columns:
        merged['sentiment'] = merged['sentiment'].fillna(0)
    return merged


def save_processed_data(df: pd.DataFrame, path: Optional[str] = None) -> None:
    """Save the merged data to a CSV file.

    Args:
        df: The merged DataFrame to save.
        path: Optional path to the output CSV. Defaults to
            ``PROCESSED_DATA_DIR/merged_data.csv``.

    Raises:
        IOError: If the file cannot be written.
    """
    if path is None:
        path = os.path.join(PROCESSED_DATA_DIR, 'merged_data.csv')
    logger.info("Saving processed data to %s", path)
    # Ensure the directory exists
    os.makedirs(os.path.dirname(path), exist_ok=True)
    try:
        df.to_csv(path, index=False)
    except Exception as exc:
        logger.exception("Failed to save processed data to %s", path)
        raise IOError(f"Failed to save processed data: {exc}")


def run_etl() -> None:
    """Execute the complete ETL pipeline.

    This function ties together all of the loading, preprocessing, merging
    and saving steps. It is intended to be called from the command line
    or used as a utility within other scripts.
    """
    try:
        price_df_raw = load_price_data()
        price_df_clean = preprocess_price_data(price_df_raw)
        news_df_raw = load_news_data()
        news_df_clean = preprocess_news_data(news_df_raw)
        merged = merge_data(price_df_clean, news_df_clean)
        save_processed_data(merged)
        logger.info("ETL pipeline completed successfully.")
    except Exception as exc:
        logger.error("ETL pipeline failed: %s", exc)
        raise


if __name__ == '__main__':  # pragma: no cover
    run_etl()
